{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Dog\n",
      "https://en.wikipedia.org/wiki/Wolf\n",
      "https://en.wikipedia.org/wiki/Cat\n",
      "https://en.wikipedia.org/wiki/Cheetah\n",
      "https://en.wikipedia.org/wiki/Leopard\n",
      "https://en.wikipedia.org/wiki/Tiger\n",
      "https://en.wikipedia.org/wiki/Lion\n",
      "https://en.wikipedia.org/wiki/Mammal\n",
      "https://en.wikipedia.org/wiki/Horse\n",
      "https://en.wikipedia.org/wiki/Elephant\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import re\n",
    "from math import log\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "porter=PorterStemmer()\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    sr=stopwords.words('english')\n",
    "    clean_tokens=tokens[:]\n",
    "    for token in tokens:\n",
    "        if token in sr:\n",
    "            clean_tokens.remove(token)\n",
    "    return clean_tokens\n",
    "\n",
    "def termFrequency(query,text):\n",
    "    normalizeTermFreq=text.lower().split()  \n",
    "    term_in_document=normalizeTermFreq.count(query.lower())  \n",
    "    len_of_document=float(len(normalizeTermFreq))  \n",
    "    normalized_tf=term_in_document/len_of_document  \n",
    "  \n",
    "    return normalized_tf \n",
    "    \n",
    "def inverseDocumentFrequency(query,text):\n",
    "    num_docs_with_given_term=0\n",
    "    if query in text: \n",
    "        num_docs_with_given_term+=1\n",
    "  \n",
    "    if num_docs_with_given_term>0: \n",
    "        total_num_docs=len(text)    \n",
    "        idf_val=log(float(total_num_docs) / num_docs_with_given_term) \n",
    "        return idf_val \n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "def index_url(url_list,query):\n",
    "    ranking_results={}\n",
    "    for url in url_list:\n",
    "        url_file=urllib.request.urlopen(url)\n",
    "        url_data=url_file.readlines()\n",
    "        #print(url_data)\n",
    "        url_file=urllib.request.urlopen(url)\n",
    "        url_str = url_file.read().lower()\n",
    "        url_wordlist = url_str.split()\n",
    "    \n",
    "    \n",
    "        #to extract raw data from web pages\n",
    "        soup=BeautifulSoup(url_str,'html5lib')\n",
    "        text=soup.get_text(strip=True)\n",
    "        #print(text)\n",
    "    \n",
    "        #to get clean data\n",
    "        clean_text=re.sub(\"[^a-zA-Z]\",\" \",text)\n",
    "        #print(clean_text)\n",
    "    \n",
    "        tokens=[t for t in clean_text.split()]\n",
    "        #print(tokens)\n",
    "    \n",
    "        #removal of stopwords\n",
    "        clean_tokens=remove_stopwords(tokens)\n",
    "        #print(clean_tokens)\n",
    "    \n",
    "        #stemming\n",
    "        stem_url=[]\n",
    "        for word in tokens:\n",
    "            stem_url.append(porter.stem(word))\n",
    "            stem_url.append(\" \")\n",
    "        #print(\"\".join(stem_url))\n",
    "        \n",
    "        #storing bag of words for each page\n",
    "        X=vectorizer.fit_transform(clean_tokens) \n",
    "        #print(X.toarray())\n",
    "        #print(vectorizer.get_feature_names())\n",
    "    \n",
    "        tf=termFrequency(query,text)\n",
    "        idf=inverseDocumentFrequency(query,text)\n",
    "    \n",
    "        scoring=tf*idf\n",
    "        #print(scoring)\n",
    "    \n",
    "        ranking_results[scoring]=url\n",
    "    \n",
    "    for i in sorted(ranking_results,reverse=True):\n",
    "        print(ranking_results[i],end=\"\\n\")\n",
    "\n",
    "\n",
    "query=\"dog\"\n",
    "url_list=['https://en.wikipedia.org/wiki/Dog','https://en.wikipedia.org/wiki/Horse','https://en.wikipedia.org/wiki/Mammal',\n",
    "          'https://en.wikipedia.org/wiki/Cat','https://en.wikipedia.org/wiki/Lion','https://en.wikipedia.org/wiki/Tiger',\n",
    "          'https://en.wikipedia.org/wiki/Cheetah','https://en.wikipedia.org/wiki/Leopard','https://en.wikipedia.org/wiki/Elephant',\n",
    "         'https://en.wikipedia.org/wiki/Wolf']\n",
    "index_url(url_list,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
